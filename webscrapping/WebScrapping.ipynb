{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit071105/Machine-Learning-/blob/main/webscrapping/WebScrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "hfi6tCYEfSaZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-6MaXMwuO6u"
      },
      "outputs": [],
      "source": [
        "# first scrappping\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "response = requests.get('https://www.olx.in/en-in', headers=headers) # web scrappping the data\n",
        "print(response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2w_roJSfUUi"
      },
      "source": [
        "\n",
        "# **if response code is 403**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6mpp6fWLU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'} -requests.get('url',headers=headers).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCNNekwqt4NX"
      },
      "outputs": [],
      "source": [
        "webpage=requests.get('https://www.ambitionbox.com/list-of-companies?page=1').text\n",
        "soup=BeautifulSoup(webpage,'lxml') #through lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrRkHBO_nWfF"
      },
      "outputs": [],
      "source": [
        "# Second scrappping\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get('https://www.ambitionbox.com/list-of-companies?page=1', headers=headers)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RlSpSgMf5Yb"
      },
      "outputs": [],
      "source": [
        "soup=BeautifulSoup(webpage,'lxml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3_tPX681f7pu",
        "outputId": "208d5f71-78a4-4187-ab9f-c3c6cbbe2b9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Access Denied'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#print(soup.prettify())\n",
        "soup.find_all('h1')[0].text # printing the text inside h1 tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqeCSw6lgDYm"
      },
      "source": [
        "# **TO FIND OUT NAMES OF THE COMPANIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORa8Bosaf_W-"
      },
      "outputs": [],
      "source": [
        "# printing all the  text inside h2 tag presented in entire site....with removing all the br , hr tag basically space tags\n",
        "for i in soup.find_all('h2'):\n",
        "  print(i.text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrEVBCI6gFCB"
      },
      "outputs": [],
      "source": [
        "# printing all the  text inside P tag presented in entire site....with removing all the br , hr tag basically space tags\n",
        "for i in soup.find_all('p'):\n",
        "  print(i.text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvqHuT3wgIAG"
      },
      "outputs": [],
      "source": [
        "# calculating the length  of p tag with class rating\n",
        "len(soup.find_all('p',class_='rating'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-w61mbfgUBZ"
      },
      "outputs": [],
      "source": [
        "# calculating the length of a tag with class review-count\n",
        "len(soup.find_all('a' , class_='review-count'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nu2pFL9gVCu"
      },
      "outputs": [],
      "source": [
        "\n",
        "company=soup.find_all('div',class_='company-content-wrapper')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtaVjECpgWhy"
      },
      "outputs": [],
      "source": [
        "len(company)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lprPE-kqgYLR"
      },
      "outputs": [],
      "source": [
        "name=[] # desiging name list to fetch data of all 30 containers\n",
        "rating=[]# desiging rating list to fetch data of all 30 containers\n",
        "reviews=[]# desiging reviews list to fetch data of all 30 containers\n",
        "ctype=[]# desiging company type list to fetch data of all 30 containers\n",
        "hq=[]# desiging headquater list to fetch data of all 30 containers\n",
        "how_old=[]# desiging employees age list to fetch data of all 30 containers\n",
        "no_of_employee=[]# employees name list to fetch data of all 30 containers\n",
        "desc=[] # description of company  list to fetch data of all 30 containers\n",
        "for i in company:\n",
        "\n",
        "  name.append(i.find('h2').text.strip()) # fetching all the name\n",
        "  rating.append(i.find('p',class_='rating').text.strip()) # fetching all the rating\n",
        "  reviews.append(i.find('a' , class_='review-count').text.strip()) # fetching all the reviews\n",
        "  ctype.append(i.find_all('p',class_='infoEntity')[0].text.strip())# fetching all the company type\n",
        "  hq.append(i.find_all('p',class_='infoEntity')[1].text.strip())# fetching all the headquater\n",
        "  how_old.append(i.find_all('p',class_='infoEntity')[2].text.strip())# fetching all the employees age\n",
        "  no_of_employee.append(i.find_all('p',class_='infoEntity')[3].text.strip())# fetching all the employees\n",
        "  desc.append(i.find_all('p',class_='description')[4].text.strip())# fetching all the employees\n",
        "  d= {'name':name,\n",
        "    'rating':rating,\n",
        "    'reviews':reviews,\n",
        "    'company_type':ctype,\n",
        "    'Head_Quarters':hq,\n",
        "    'Company_Age':how_old,\n",
        "    'No_of_Employee':no_of_employee,\n",
        "      } # fetching through dictonary\n",
        "  p=pd.DataFrame(d) # showing all the data\n",
        "\n",
        "df=pd.DataFrame({'name':name,\n",
        "   'rating':rating,\n",
        "   'reviews':reviews,\n",
        "   'company_type':ctype,\n",
        "   'Head_Quarters':hq,\n",
        "   'Company_Age':how_old,\n",
        "   'No_of_Employee':no_of_employee,\n",
        "   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVIhBinZgZzd"
      },
      "outputs": [],
      "source": [
        "name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT5hDt1igf2L"
      },
      "outputs": [],
      "source": [
        "rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMtI5VuDghLE"
      },
      "outputs": [],
      "source": [
        "reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKR8IS-Ygk-A"
      },
      "outputs": [],
      "source": [
        "ctype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8P83aHqgl7d"
      },
      "outputs": [],
      "source": [
        "hq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krsAzmR3gnTz"
      },
      "outputs": [],
      "source": [
        "how_old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4mwgL3-gozW"
      },
      "outputs": [],
      "source": [
        "no_of_employee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xoL4cmegqQ0"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44T6q5KsgspL"
      },
      "source": [
        "\n",
        "# **creating dataframe for all the pages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0KuApT5grbE"
      },
      "outputs": [],
      "source": [
        "final=pd.DataFrame()\n",
        "for j in range(1,1001):\n",
        "  webpage=requests.get('https://www.ambitionbox.com/list-of-companies?page={}'.format(j)).text\n",
        "  soup=BeautifulSoup(webpage,'lxml')\n",
        "  company=soup.find_all('div',class_='company-content-wrapper')\n",
        "  name=[]\n",
        "  rating=[]\n",
        "  reviews=[]\n",
        "  ctype=[]\n",
        "  hq=[]\n",
        "  how_old=[]\n",
        "  no_of_employee=[]\n",
        "\n",
        "  for i in company:\n",
        "\n",
        "    try:\n",
        "       name.append(i.find('h2').text.strip())\n",
        "    except:\n",
        "       name.append(np.nan)\n",
        "\n",
        "    try:\n",
        "       rating.append(i.find('p',class_='rating').text.strip())\n",
        "    except:\n",
        "       rating.append(np.nan)\n",
        "\n",
        "    try:\n",
        "\n",
        "      reviews.append(i.find('a' , class_='review-count').text.strip())\n",
        "    except:\n",
        "      reviews.append(np.nan)\n",
        "\n",
        "    try:\n",
        "\n",
        "      ctype.append(i.find_all('p',class_='infoEntity')[0].text.strip())\n",
        "    except:\n",
        "      ctype.append(np.nan)\n",
        "    try:\n",
        "\n",
        "      hq.append(i.find_all('p',class_='infoEntity')[1].text.strip())\n",
        "    except:\n",
        "      hq.append(np.nan)\n",
        "\n",
        "    try:\n",
        "\n",
        "      how_old.append(i.find_all('p',class_='infoEntity')[2].text.strip())\n",
        "    except:\n",
        "      how_old.append(np.nan)\n",
        "    try:\n",
        "      no_of_employee.append(i.find_all('p',class_='infoEntity')[3].text.strip())\n",
        "    except:\n",
        "      no_of_employee.append(np.nan)\n",
        "\n",
        "\n",
        "  df=pd.DataFrame({'name':name,\n",
        "    'rating':rating,\n",
        "    'reviews':reviews,\n",
        "    'company_type':ctype,\n",
        "    'Head_Quarters':hq,\n",
        "    'Company_Age':how_old,\n",
        "    'No_of_Employee':no_of_employee,\n",
        "    })\n",
        "\n",
        "  final=final.append(df,ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkhuY82whNXd"
      },
      "outputs": [],
      "source": [
        "all_dfs = []  # collect dataframes here\n",
        "\n",
        "for j in range(1, 1001):\n",
        "    print(f\"Scraping page {j}...\")  # optional progress\n",
        "    webpage = requests.get(f'https://www.ambitionbox.com/list-of-companies?page={j}').text # fetching all the page data j :shows the page number\n",
        "    soup = BeautifulSoup(webpage, 'lxml')\n",
        "    company = soup.find_all('div', class_='company-content-wrapper') # big container - total container 30\n",
        "\n",
        "    name = []\n",
        "    rating = []\n",
        "    reviews = []\n",
        "    ctype = []\n",
        "    hq = []\n",
        "    how_old = []\n",
        "    no_of_employee = []\n",
        "   # using try and catch such that ignore all the other error\n",
        "    for i in company:\n",
        "        try:\n",
        "            name.append(i.find('h2').text.strip())\n",
        "        except:\n",
        "            name.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            rating.append(i.find('p', class_='rating').text.strip())\n",
        "        except:\n",
        "            rating.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            reviews.append(i.find('a', class_='review-count').text.strip())\n",
        "        except:\n",
        "            reviews.append(np.nan)\n",
        "\n",
        "        info_entities = i.find_all('p', class_='infoEntity')\n",
        "        try:\n",
        "            ctype.append(info_entities[0].text.strip())\n",
        "        except:\n",
        "            ctype.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            hq.append(info_entities[1].text.strip())\n",
        "        except:\n",
        "            hq.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            how_old.append(info_entities[2].text.strip())\n",
        "        except:\n",
        "            how_old.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            no_of_employee.append(info_entities[3].text.strip())\n",
        "        except:\n",
        "            no_of_employee.append(np.nan)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'name': name,\n",
        "        'rating': rating,\n",
        "        'reviews': reviews,\n",
        "        'company_type': ctype,\n",
        "        'Head_Quarters': hq,\n",
        "        'Company_Age': how_old,\n",
        "        'No_of_Employee': no_of_employee,\n",
        "    })\n",
        "\n",
        "    all_dfs.append(df)  # append dataframe to the list\n",
        "\n",
        "# concatenate all dataframes into one final dataframe\n",
        "final = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# optionally save to CSV\n",
        "final.to_csv('ambitionbox_companies.csv', index=False)\n",
        "print(\"Scraping completed and data saved to ambitionbox_companies.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yHqNafGgzJm"
      },
      "outputs": [],
      "source": [
        "df.sample(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW0nw19jg17c"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Sf5ZGZg4wx"
      },
      "outputs": [],
      "source": [
        "final.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp8S44iHIwabc013NucuxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}